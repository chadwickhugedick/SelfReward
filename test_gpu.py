#!/usr/bin/env python3
"""
GPU Utilization Test for SRDDQN Training
Check if GPU is being properly utilized during training
"""

import torch
import time
import os

def test_gpu_tensor_operations():
    """Test basic GPU tensor operations"""
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available!")
        return False
    
    print("‚úÖ CUDA available!")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Test tensor operations on GPU
    print("\nüî• Testing GPU tensor operations...")
    device = torch.device('cuda')
    
    # Create large tensors on GPU
    x = torch.randn(1000, 1000, device=device)
    y = torch.randn(1000, 1000, device=device)
    
    start_time = time.time()
    for i in range(100):
        z = torch.matmul(x, y)
        torch.cuda.synchronize()  # Wait for GPU operations to complete
    
    gpu_time = time.time() - start_time
    print(f"GPU computation time: {gpu_time:.3f} seconds")
    
    # Test CPU for comparison
    print("\nüêå Testing CPU tensor operations...")
    x_cpu = x.cpu()
    y_cpu = y.cpu()
    
    start_time = time.time()
    for i in range(100):
        z_cpu = torch.matmul(x_cpu, y_cpu)
    
    cpu_time = time.time() - start_time
    print(f"CPU computation time: {cpu_time:.3f} seconds")
    print(f"GPU speedup: {cpu_time/gpu_time:.1f}x")
    
    return True

def check_gpu_memory():
    """Check GPU memory usage"""
    if torch.cuda.is_available():
        print(f"\nüìä GPU Memory Status:")
        print(f"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
        print(f"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")
        print(f"Max allocated: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB")
        
        # Clear cache
        torch.cuda.empty_cache()
        print("GPU cache cleared")

def main():
    print("üöÄ SRDDQN GPU Utilization Test")
    print("=" * 60)
    
    # Test basic GPU functionality
    if not test_gpu_tensor_operations():
        return
    
    check_gpu_memory()
    
    print("\n" + "=" * 60)
    print("üí° GPU Optimization Tips for SRDDQN:")
    print("=" * 60)
    
    # Check current configuration
    if os.path.exists('configs/srddqn_config.yaml'):
        with open('configs/srddqn_config.yaml', 'r') as f:
            content = f.read()
            
        print("\nüìä Current Configuration Analysis:")
        if 'batch_size: 64' in content:
            print("‚úÖ Training batch size: 64 (good for GPU)")
        elif 'batch_size: 128' in content:  
            print("‚úÖ Pretraining batch size: 128 (good for GPU)")
        else:
            print("‚ö†Ô∏è  Small batch sizes detected - GPU may be underutilized")
            
        if 'mixed_precision: true' in content:
            print("‚úÖ Mixed precision enabled")
        else:
            print("‚ö†Ô∏è  Mixed precision not enabled")
    
    print("\nüîß Optimizations Applied:")
    print("‚úÖ Mixed precision training")
    print("‚úÖ Non-blocking GPU transfers") 
    print("‚úÖ Multi-worker data loading (num_workers=4)")
    print("‚úÖ Pinned memory for faster transfers")
    print("‚úÖ Increased batch sizes")
    
    print("\nüöÄ Your system should now utilize GPU much better!")
    print("Run training and watch for faster episode completion times.")

if __name__ == "__main__":
    main()